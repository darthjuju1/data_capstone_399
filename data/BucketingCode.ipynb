{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "904a76f8-2c88-4c2d-a0ee-5607136e4fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined script containing all code from the three notebooks\n",
    "\n",
    "# --- Begin query_pinecone (2) Notebook ---\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "#connect to an existing index\n",
    "\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=\"pcsk_3StWu1_F4vFjfS4BMLmpvfuBE6aUyYE1LRVGetymK5Q4FHRBCFNbJWK2KRjtn1insArDth\") # \"New API key as of 4/18/2025\"\n",
    "\n",
    "index = pc.Index(name=\"test-index\", host=\"https://test-index-9l7icfb.svc.aped-4627-b74a.pinecone.io\")  # the name you gave it\n",
    "\n",
    "#creating a new index\n",
    "\n",
    "#  Import the Pinecone library\n",
    "from pinecone import Pinecone\n",
    "\n",
    "# Initialize a Pinecone client with your API key\n",
    "pc = Pinecone(api_key=\"pcsk_3StWu1_F4vFjfS4BMLmpvfuBE6aUyYE1LRVGetymK5Q4FHRBCFNbJWK2KRjtn1insArDth\") # \"New API key as of 4/18/2025\"\n",
    "\n",
    "# Create a dense index with integrated embedding\n",
    "index_name = \"test-index\"\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index_for_model(\n",
    "        name=index_name,\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\",\n",
    "        embed={\n",
    "            \"model\":\"llama-text-embed-v2\",\n",
    "            \"field_map\":{\"text\": \"chunk_text\"}\n",
    "        }\n",
    "    )\n",
    "\n",
    "pc.describe_index(name=index_name)\n",
    "\n",
    "test_index = pc.Index(name=\"test-index\", host=\"https://test-index-mzzv42s.svc.aped-4627-b74a.pinecone.io\")  # the name you gave it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce007baf-62fe-44a1-9d87-732e418a025b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total songs: 12500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Track Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Year</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>Spotify Genres</th>\n",
       "      <th>Release Date</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Versace on the Floor</td>\n",
       "      <td>Bruno Mars</td>\n",
       "      <td>dance pop,pop</td>\n",
       "      <td>11/17/2016</td>\n",
       "      <td>Lets take our time tonight girl Above us all ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11/17/2016</td>\n",
       "      <td>78</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If I Never See Your Face Again Remix</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>pop,barbadian pop,pop,urban contemporary</td>\n",
       "      <td>6/2/2008</td>\n",
       "      <td>Now as the summer fades I let you slip away Y...</td>\n",
       "      <td>pop</td>\n",
       "      <td>6/2/2008</td>\n",
       "      <td>49</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When I Was Young</td>\n",
       "      <td>The Animals</td>\n",
       "      <td>british invasion,classic rock,folk rock</td>\n",
       "      <td>12/2/2005</td>\n",
       "      <td>The rooms were so much colder then My fathe...</td>\n",
       "      <td>psychedelic rock, acid rock</td>\n",
       "      <td>6/11/1991</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dizzy</td>\n",
       "      <td>Tommy Roe</td>\n",
       "      <td>bubblegum pop,classic garage rock,merseybeat,r...</td>\n",
       "      <td>1/1/1977</td>\n",
       "      <td>Dizzy Im so dizzy my head is spinning Like a w...</td>\n",
       "      <td>bubblegum pop</td>\n",
       "      <td>1/1/2001</td>\n",
       "      <td>50</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>When Youre Gone</td>\n",
       "      <td>Bryan Adams</td>\n",
       "      <td>canadian pop,canadian singer-songwriter,classi...</td>\n",
       "      <td>1/1/1998</td>\n",
       "      <td>When youre gone   Ive been wandering around t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/1/1998</td>\n",
       "      <td>67</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Track Name  Artist Name  \\\n",
       "0                  Versace on the Floor   Bruno Mars   \n",
       "1  If I Never See Your Face Again Remix     Maroon 5   \n",
       "2                      When I Was Young  The Animals   \n",
       "3                                 Dizzy    Tommy Roe   \n",
       "4                       When Youre Gone  Bryan Adams   \n",
       "\n",
       "                                               Genre        Year  \\\n",
       "0                                      dance pop,pop  11/17/2016   \n",
       "1           pop,barbadian pop,pop,urban contemporary    6/2/2008   \n",
       "2            british invasion,classic rock,folk rock   12/2/2005   \n",
       "3  bubblegum pop,classic garage rock,merseybeat,r...    1/1/1977   \n",
       "4  canadian pop,canadian singer-songwriter,classi...    1/1/1998   \n",
       "\n",
       "                                              Lyrics  \\\n",
       "0   Lets take our time tonight girl Above us all ...   \n",
       "1   Now as the summer fades I let you slip away Y...   \n",
       "2     The rooms were so much colder then My fathe...   \n",
       "3  Dizzy Im so dizzy my head is spinning Like a w...   \n",
       "4   When youre gone   Ive been wandering around t...   \n",
       "\n",
       "                Spotify Genres Release Date  Popularity  Explicit  \n",
       "0                          NaN   11/17/2016          78     False  \n",
       "1                          pop     6/2/2008          49     False  \n",
       "2  psychedelic rock, acid rock    6/11/1991          32     False  \n",
       "3                bubblegum pop     1/1/2001          50     False  \n",
       "4                          NaN     1/1/1998          67     False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the new, larger CSV\n",
    "clean_lyrics = pd.read_csv(r\"song_lyrics (4).csv\")\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Total songs:\", len(clean_lyrics))\n",
    "display(clean_lyrics.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f07d9a3-b1b0-4f9f-91b8-e6126f2b3394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    rec0\n",
      "1    rec1\n",
      "2    rec2\n",
      "3    rec3\n",
      "4    rec4\n",
      "5    rec5\n",
      "6    rec6\n",
      "7    rec7\n",
      "8    rec8\n",
      "9    rec9\n",
      "Name: _id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = clean_lyrics.copy()\n",
    "df['_id'] = 'rec' + df.index.astype(str)\n",
    "\n",
    "# Peek first few IDs\n",
    "print(df['_id'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "214fa35a-c35d-4903-9af8-3e466e399e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/px/kgbj015s0mx1vfbs8ptpm0l80000gn/T/ipykernel_72465/1543485848.py:11: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, infer_datetime_format=True, errors=\"coerce\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-11-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-06-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2005-12-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1977-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1998-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Year\n",
       "0  2016-11-17\n",
       "1  2008-06-02\n",
       "2  2005-12-02\n",
       "3  1977-01-01\n",
       "4  1998-01-01"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_year_field(s):\n",
    "    if pd.isna(s): return pd.NaT\n",
    "    s = str(s).strip()\n",
    "    if re.fullmatch(r\"\\d{4}\", s):\n",
    "        return pd.to_datetime(f\"{s}-01-01\")\n",
    "    if re.fullmatch(r\"\\d{4}-\\d{1,2}\", s):\n",
    "        year, month = s.split(\"-\")\n",
    "        return pd.to_datetime(f\"{year}-{month.zfill(2)}-01\")\n",
    "    return pd.to_datetime(s, infer_datetime_format=True, errors=\"coerce\")\n",
    "\n",
    "df['Year'] = df['Year'].apply(parse_year_field)\n",
    "df['Year'] = df['Year'].dt.strftime('%Y-%m-%d')\n",
    "display(df[['Year']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e88af33b-bdee-4a42-8737-c6a9a9334842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded cleaned file, rows: 12500\n"
     ]
    }
   ],
   "source": [
    "df.to_csv('v2_cleaned_song_lyrics.csv', index=False)\n",
    "df = pd.read_csv(\"v2_cleaned_song_lyrics.csv\")\n",
    "print(\"Reloaded cleaned file, rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78f81d61-390b-4610-a722-3a22bae3eed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 12500 records for upsert.\n"
     ]
    }
   ],
   "source": [
    "records = (\n",
    "    df[['_id', 'Lyrics', 'Genre', 'Year', 'Track Name', 'Artist Name']]\n",
    "      .rename(columns={\n",
    "         'Lyrics': 'chunk_text',\n",
    "         'Genre' : 'genre',\n",
    "         'Year'  : 'year',\n",
    "         'Track Name'  : 'track_name',\n",
    "         'Artist Name' : 'artist_name'\n",
    "      })\n",
    "    .to_dict(orient='records')\n",
    ")\n",
    "\n",
    "print(\"Prepared\", len(records), \"records for upsert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b39c252c-56ad-4e58-92ab-6458ae21cb4c",
   "metadata": {},
   
    
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1) Embed locally\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "for rec in records:\n",
    "    rec[\"values\"] = model.encode(rec[\"chunk_text\"]).tolist()\n",
    "\n",
    "# 2) Batch upsert raw vectors\n",
    "batch_size = 100\n",
    "for i in range(0, len(records), batch_size):\n",
    "    batch = records[i : i + batch_size]\n",
    "    upserts = [\n",
    "        {\n",
    "          \"id\": r[\"_id\"],\n",
    "          \"values\": r[\"values\"],\n",
    "          \"metadata\": {\n",
    "            \"genre\":      r[\"genre\"],\n",
    "            \"year\":       r[\"year\"],\n",
    "            \"track_name\": r[\"track_name\"],\n",
    "            \"artist_name\":r[\"artist_name\"]\n",
    "          }\n",
    "        }\n",
    "        for r in batch\n",
    "    ]\n",
    "    test_index.upsert(vectors=upserts)\n",
    "    print(f\"Upserted {i + len(batch)}/{len(records)} records\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7236dc8c-6a9e-4563-89d8-a8d63bd3421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from ast import literal_eval\n",
    "\n",
    "# now ready for TF-IDF, vector parsing, buckets, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f75e6d-b5bf-4826-b5e8-1fba5e572089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load metadata‐rich subset\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "df[\"MetaGenres_list\"] = df[\"MetaGenres\"].apply(literal_eval)\n",
    "has_meta = df[df[\"MetaGenres_list\"].apply(lambda L: L != [\"OtherMeta\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f37490-eb93-4f72-8ee8-f0fd9a9e4c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Fit a TF–IDF vectorizer (unigrams only, English stop‐words)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    stop_words=\"english\",\n",
    "    ngram_range=(1,1)\n",
    ")\n",
    "X_tfidf = vectorizer.fit_transform(has_meta[\"Lyrics\"].astype(str))\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "print(f\"TF–IDF matrix shape: {X_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff84a423-5dc7-44a8-82e7-0fd130e7475f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms = {}\n",
    "for bucket in sorted(has_meta[\"Bucket\"].unique()):\n",
    "    mask = has_meta[\"Bucket\"] == bucket\n",
    "    # mean tf–idf score of each term within that bucket\n",
    "    mean_tfidf = X_tfidf[mask].mean(axis=0).A1\n",
    "    top20_idx = np.argsort(mean_tfidf)[-20:][::-1]\n",
    "    top_terms[bucket] = [terms[i] for i in top20_idx]\n",
    "\n",
    "# Peek at the first 10 terms for each bucket\n",
    "for bucket, words in top_terms.items():\n",
    "    print(f\"{bucket:12s} → {', '.join(words[:10])} …\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43ee544-4113-4c54-81e9-828cfcb851dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "themes = [\"love\", \"party\", \"breakup\"]\n",
    "all_hits = []\n",
    "\n",
    "for theme in themes:\n",
    "    res = test_index.search(\n",
    "        namespace=\"test-namespace\",\n",
    "        query={\"inputs\": {\"text\": theme}, \"top_k\": 20},\n",
    "        fields=[\"chunk_text\"],\n",
    "        rerank={\n",
    "            \"model\":\"bge-reranker-v2-m3\",\n",
    "            \"top_n\":5,\n",
    "            \"rank_fields\":[\"chunk_text\"]\n",
    "        }\n",
    "    )\n",
    "    for m in res[\"matches\"]:\n",
    "        all_hits.append({\n",
    "            \"theme\": theme,\n",
    "            \"song_id\": m[\"id\"],\n",
    "            \"score\": m[\"score\"]\n",
    "        })\n",
    "\n",
    "df_themes = pd.DataFrame(all_hits)\n",
    "print(df_themes.groupby(\"theme\")[\"score\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02238f70-21dd-4304-9401-bd34436bd91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_with_text = test_index.search(\n",
    "    namespace=\"test-namespace\",\n",
    "    query={\"inputs\": {\"text\": \"Dancing Clowns\"}, \"top_k\": 4},\n",
    "    fields=[\"category\", \"chunk_text\"],\n",
    "    rerank={\n",
    "        \"model\": \"bge-reranker-v2-m3\",\n",
    "        \"top_n\": 2,\n",
    "        \"rank_fields\": [\"chunk_text\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(search_with_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500bbca-79d7-43f0-9304-dcd6c13aefa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# 1) Load & prepare data\n",
    "clean_lyrics = pd.read_csv(\"v2_cleaned_song_lyrics.csv\")\n",
    "clean_lyrics[\"Year\"] = pd.to_datetime(clean_lyrics[\"Year\"], errors=\"coerce\")\n",
    "clean_lyrics = clean_lyrics.dropna(subset=[\"Year\"])\n",
    "clean_lyrics[\"Decade\"] = (clean_lyrics[\"Year\"].dt.year // 10 * 10).astype(str) + \"s\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee56573f-002f-4c71-8e98-eace97ffea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Generate SBERT embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "vectors = model.encode(clean_lyrics[\"Lyrics\"].astype(str).tolist(), show_progress_bar=True)\n",
    "clean_lyrics[\"vector\"] = vectors.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf6cf12-69dd-4ed4-8b95-8edbf6a4e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Define similarity function\n",
    "def avg_similarity(vs):\n",
    "    sim = cosine_similarity(vs)\n",
    "    ut = sim[np.triu_indices(len(sim), k=1)]\n",
    "    return ut.mean() if len(ut)>0 else 0\n",
    "\n",
    "# Compute per-decade\n",
    "decade_similarities = {}\n",
    "for dec in sorted(clean_lyrics[\"Decade\"].unique()):\n",
    "    vecs = clean_lyrics[clean_lyrics[\"Decade\"]==dec][\"vector\"].tolist()\n",
    "    if len(vecs)>=2:\n",
    "        decade_similarities[dec] = avg_similarity(vecs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8f0cca-cdd5-4bb7-80a2-5c4e38a084d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Report\n",
    "for dec, sim in decade_similarities.items():\n",
    "    print(f\"{dec}: Avg Similarity = {sim:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765dc30f-d655-402e-b847-0eef7b3e93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) Load full CSV\n",
    "df = pd.read_csv(\"song_lyrics (4).csv\")\n",
    "\n",
    "# 2) Parse Release Date\n",
    "df[\"Release Date\"] = pd.to_datetime(df[\"Release Date\"], errors=\"coerce\")\n",
    "\n",
    "print(\"Total rows:\", len(df))\n",
    "print(\"Parsed Release Dates:\", df[\"Release Date\"].notna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bb544c-a0d1-4128-9675-d77e584ff4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Extract Year & Decade\n",
    "df[\"Year\"] = df[\"Release Date\"].dt.year\n",
    "df[\"Decade\"] = (df[\"Year\"] // 10 * 10).astype(pd.Int64Dtype()).astype(str) + \"s\"\n",
    "\n",
    "# 4) Mark unparsable as \"Unknown\"\n",
    "df.loc[df[\"Release Date\"].isna(), \"Decade\"] = \"Unknown\"\n",
    "\n",
    "# 5) View counts\n",
    "print(df[\"Decade\"].value_counts(dropna=False).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478e8bcc-fc88-4c5b-99af-141a942994a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) See what raw strings failed to parse\n",
    "raw = pd.read_csv(\"song_lyrics (4).csv\")\n",
    "raw[\"Release Date_raw\"] = raw[\"Release Date\"].astype(str)\n",
    "bad = raw[pd.to_datetime(raw[\"Release Date\"], errors=\"coerce\").isna()]\n",
    "print(\"Sample bad raws:\", bad[\"Release Date_raw\"].unique()[:20])\n",
    "\n",
    "# 7) Extract first 4-digit year & rebuild Decade\n",
    "df[\"Year_extracted\"] = (\n",
    "    raw[\"Release Date_raw\"]\n",
    "      .str.extract(r\"(\\d{4})\")[0]\n",
    "      .astype(float)\n",
    ")\n",
    "df[\"Decade\"] = (df[\"Year_extracted\"].dropna().astype(int)//10*10)\n",
    "df[\"Decade\"] = df[\"Decade\"].astype(pd.Int64Dtype()).astype(str)+\"s\"\n",
    "df.loc[df[\"Year_extracted\"].isna(), \"Decade\"] = \"Unknown\"\n",
    "\n",
    "print(df[\"Decade\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "403c4c47-ebdc-48e7-aee6-18c908b24840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load full CSV\n",
    "df = pd.read_csv(\"song_lyrics (4).csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad76e730-dc42-48ba-a501-e258b6eb591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse Release Date into datetime (invalid→NaT)\n",
    "df[\"Release Date\"] = pd.to_datetime(df[\"Release Date\"], errors=\"coerce\")\n",
    "\n",
    "# Extract Year & Decade, defaulting to “Unknown” if missing\n",
    "df[\"Year\"] = df[\"Release Date\"].dt.year\n",
    "df[\"Decade\"] = df[\"Release Date\"].apply(\n",
    "    lambda ts: f\"{(ts.year//10*10)}s\" if pd.notna(ts) else \"Unknown\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31ff0e-2570-462c-9a21-2a6b2afe80ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split comma-delimited Spotify Genres into lists ([\"OtherMeta\"] if empty)\n",
    "df[\"MetaGenres\"] = (\n",
    "    df[\"Spotify Genres\"].fillna(\"\")\n",
    "      .str.split(\",\")\n",
    "      .apply(lambda L: [g.strip() for g in L if g.strip()] or [\"OtherMeta\"])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83390322-69a5-452f-99ef-e3e98a8b221f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta_bucket(genres):\n",
    "    # genres is a list of strings\n",
    "    buckets = [map_bucket(g) for g in genres]\n",
    "    # take the most common bucket among them\n",
    "    return pd.Series(buckets).mode()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9e185-cb5a-4a61-820c-5ecacf568b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into “has metadata” vs “no metadata”\n",
    "has_meta = df[df[\"MetaGenres\"].apply(lambda L: L != [\"OtherMeta\"])].copy()\n",
    "no_meta  = df[df[\"MetaGenres\"].apply(lambda L: L == [\"OtherMeta\"])].copy()\n",
    "\n",
    "# Assign Bucket for those with metadata\n",
    "has_meta[\"Bucket\"] = has_meta[\"MetaGenres\"].apply(assign_meta_bucket)\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"Buckets in has_meta:\\n\", has_meta[\"Bucket\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2b9ae9-a698-4fe6-9e7c-ff7d5759c0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# 1) Fit k-NN on known songs’ SBERT vectors\n",
    "X_meta = np.stack(has_meta[\"vector\"].tolist())\n",
    "nn     = NearestNeighbors(n_neighbors=5, metric=\"cosine\").fit(X_meta)\n",
    "\n",
    "# 2) Find neighbors for the missing-metadata songs\n",
    "X_no = np.stack(no_meta[\"vector\"].tolist())\n",
    "_, idxs = nn.kneighbors(X_no)\n",
    "\n",
    "# 3) Predict each missing one as the modal bucket of its neighbors\n",
    "preds = []\n",
    "for neigh_idxs in idxs:\n",
    "    neigh_buckets = has_meta.iloc[neigh_idxs][\"Bucket\"]\n",
    "    preds.append(neigh_buckets.mode()[0])\n",
    "\n",
    "no_meta[\"Bucket\"] = preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b837d2-5a1d-4c73-a520-70f666513ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two back into the main DataFrame\n",
    "df.loc[has_meta.index, \"Bucket\"] = has_meta[\"Bucket\"]\n",
    "df.loc[ no_meta.index, \"Bucket\"] = no_meta[\"Bucket\"]\n",
    "\n",
    "# 1) No more missing buckets\n",
    "print(\"Missing buckets:\", df[\"Bucket\"].isna().sum())\n",
    "\n",
    "# 2) Overall bucket distribution\n",
    "print(\"Bucket counts:\\n\", df[\"Bucket\"].value_counts())\n",
    "\n",
    "# 3) key columns\n",
    "print(\n",
    "    df[[\"Track Name\",\"Artist Name\",\"MetaGenres\",\"Bucket\",\"Decade\"]]\n",
    "      .head(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c6a2ca-8ab7-42df-813c-14e9e261a416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "# Assuming `df` is your merged DataFrame with 'Bucket'\n",
    "df.to_csv(\"F-all_songs_merged_buckets.csv\", index=False)\n",
    "display(FileLink(\"F-all_songs_merged_buckets.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcb75a6-4ea7-4589-82b9-33de0536a26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_merged_buckets.csv\")\n",
    "display(df.head())\n",
    "print(\"Songs per bucket:\\n\", df[\"Bucket\"].value_counts().sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060164e2-0b83-43b4-844e-5aaa14432b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Subset “Other”\n",
    "other = df[df[\"Bucket\"] == \"Other\"].copy()\n",
    "\n",
    "# 2) Count granular MetaGenres\n",
    "print(\"Top granular genres in Other:\")\n",
    "print(other[\"MetaGenres\"]\n",
    "      .explode()\n",
    "      .value_counts()\n",
    "      .head(20))\n",
    "\n",
    "# 3) Sample a few tracks\n",
    "display(other[[\"Track Name\",\"Artist Name\",\"MetaGenres\",\"Decade\"]]\n",
    "        .sample(10, random_state=42))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c823d207-201a-4498-82ef-98dace865d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update BUCKET_MAP dict for finer genres:\n",
    "BUCKET_MAP[\"Pop\"]         += [\"indie\"]  \n",
    "BUCKET_MAP[\"Rock\"]        += [\"rock\",\"glam metal\",\"alternative metal\",\"nu metal\"]  \n",
    "BUCKET_MAP[\"Electronic\"]  += [\"hi-nrg\"]  \n",
    "BUCKET_MAP[\"R&B/Soul\"]    += [\"doo-wop\",\"motown\"]  \n",
    "BUCKET_MAP[\"Country/Folk\"]+= [\"singer-songwriter\"]  \n",
    "BUCKET_MAP[\"Jazz/Blues\"]  += [\"adult standards\"]  \n",
    "BUCKET_MAP[\"Latin/World\"] += [\"hindi pop\",\"desi\",\"bollywood\",\"desi pop\",\n",
    "                               \"hindi indie\",\"variété française\",\"reggae\",\"schlager\"]  \n",
    "BUCKET_MAP[\"Other\"]       += [\"christmas\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113c988-2974-4514-86da-5e03aa59576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Re-assign buckets for has_meta\n",
    "has_meta = df[df[\"MetaGenres\"].apply(lambda L: L!=[\"OtherMeta\"])].copy()\n",
    "has_meta[\"Bucket\"] = has_meta[\"MetaGenres\"].apply(assign_meta_bucket)\n",
    "\n",
    "# 2) Merge back into df\n",
    "df.loc[has_meta.index, \"Bucket\"] = has_meta[\"Bucket\"]\n",
    "\n",
    "# 3) Save & link\n",
    "df.to_csv(\"F-all_songs_refined_buckets.csv\", index=False)\n",
    "from IPython.display import FileLink\n",
    "display(FileLink(\"F-all_songs_refined_buckets.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349e906-1632-4e8b-a69a-899d936b74bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_refined_buckets.csv\")\n",
    "\n",
    "# If PC1/PC2 missing, compute from vector column\n",
    "if not {\"PC1\",\"PC2\"}.issubset(df):\n",
    "    vectors = df[\"vector\"].apply(lambda v: eval(v) if isinstance(v,str) else v).tolist()\n",
    "    X = np.stack(vectors)\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    coords = pca.fit_transform(X)\n",
    "    df[\"PC1\"], df[\"PC2\"] = coords[:,0], coords[:,1]\n",
    "\n",
    "# Save & link\n",
    "df.to_csv(\"F-all_songs_refined_with_PCA.csv\", index=False)\n",
    "from IPython.display import FileLink, display\n",
    "display(FileLink(\"F-all_songs_refined_with_PCA.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f2439-d4c0-4d50-a475-1d67e91ff20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_refined_with_PCA.csv\")\n",
    "other = df[df[\"Bucket\"] == \"Other\"]\n",
    "\n",
    "print(\"Total ‘Other’ songs:\", len(other))\n",
    "display(other[[\"Track Name\",\"Artist Name\",\"MetaGenres\",\"PC1\",\"PC2\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f032a4f0-8910-411f-95e3-3f8c16b9b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_refined_with_PCA.csv\")\n",
    "print(\"Bucket counts:\\n\", df[\"Bucket\"].value_counts().sort_values(ascending=False))\n",
    "\n",
    "# Parse Release Date and build Decade label\n",
    "df[\"Release Date\"] = pd.to_datetime(df[\"Release Date\"], errors=\"coerce\")\n",
    "def make_decade(ts):\n",
    "    return f\"{(ts.year//10*10)}s\" if pd.notna(ts) else \"Unknown\"\n",
    "\n",
    "df[\"Decade\"] = df[\"Release Date\"].apply(make_decade)\n",
    "\n",
    "print(\"\\nDecade counts:\\n\", df[\"Decade\"].value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41817f5e-34c4-44cd-8bd0-9b158ef2ccba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_refined_with_PCA.csv\")\n",
    "# Extract first 4-digit year from string, convert→float\n",
    "df[\"Year_extracted\"] = (\n",
    "    df[\"Release Date\"].astype(str)\n",
    "      .str.extract(r\"(\\d{4})\")[0]\n",
    "      .astype(float)\n",
    ")\n",
    "# Build Decade from extracted year\n",
    "df[\"Decade\"] = df[\"Year_extracted\"].apply(\n",
    "    lambda y: f\"{int(y//10*10)}s\" if not pd.isna(y) else \"Unknown\"\n",
    ")\n",
    "\n",
    "out = \"F-all_songs_with_regex_decade.csv\"\n",
    "df.to_csv(out, index=False)\n",
    "display(FileLink(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241fb22-2be9-4610-912f-bb153e10e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "\n",
    "balanced_sets = {\n",
    "    \"F-songs_balanced_50s_2020s.csv\": ['1950s','1960s','1970s','1980s','1990s','2000s','2010s','2020s'],\n",
    "    \"F-songs_balanced_60s_2020s.csv\": ['1960s','1970s','1980s','1990s','2000s','2010s','2020s'],\n",
    "    \"F-songs_balanced_70s_2020s.csv\": ['1970s','1980s','1990s','2000s','2010s','2020s'],\n",
    "    \"F-songs_balanced_70s_2010s.csv\": ['1970s','1980s','1990s','2000s','2010s']\n",
    "}\n",
    "\n",
    "for out_file, decades in balanced_sets.items():\n",
    "    subset = df[df[\"Decade\"].isin(decades)]\n",
    "    n = subset[\"Decade\"].value_counts().min()\n",
    "    balanced = subset.groupby(\"Decade\").sample(n=n, random_state=42)\n",
    "    balanced.to_csv(out_file, index=False)\n",
    "    display(FileLink(out_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b34d2-c11a-4727-8484-fce1ffffc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "\n",
    "decades = ['1950s','1960s','1970s','1980s','1990s','2000s','2010s','2020s']\n",
    "slice_50_20 = df[df.Decade.isin(decades)]\n",
    "\n",
    "min_count = slice_50_20.Decade.value_counts().min()\n",
    "\n",
    "balanced_50_20 = (\n",
    "    slice_50_20\n",
    "      .groupby(\"Decade\", group_keys=False)\n",
    "      .apply(lambda grp: grp.sample(n=min_count, random_state=42))\n",
    ")\n",
    "\n",
    "print(balanced_50_20.Decade.value_counts().sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfee87c4-0abb-4bbc-8eb0-b65edd42dc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "files = [\n",
    "    \"F-songs_balanced_50s_2020s.csv\",\n",
    "    \"F-songs_balanced_60s_2020s.csv\",\n",
    "    \"F-songs_balanced_70s_2020s.csv\",\n",
    "    \"F-songs_balanced_70s_2010s.csv\"\n",
    "]\n",
    "\n",
    "for fn in files:\n",
    "    df_bal = pd.read_csv(fn)\n",
    "    counts = df_bal[\"Decade\"].value_counts().sort_index()\n",
    "    print(f\"\\n**{fn}**\")\n",
    "    print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7c4ab-46ab-40f2-b803-df44da035a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "\n",
    "hiphop_60s = df[\n",
    "    (df[\"Decade\"] == \"1960s\") &\n",
    "    (df[\"Bucket\"] == \"Hip-Hop/Rap\")\n",
    "]\n",
    "\n",
    "print(len(hiphop_60s))\n",
    "print(hiphop_60s[[\"Track Name\",\"Artist Name\",\"Release Date\"]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b76f3-2e02-40fe-abb8-14177c514ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "\n",
    "df = df[df[\"Decade\"].astype(str).str[:-1].astype(int) >= 1970]\n",
    "\n",
    "df[\"HipHop_Source\"] = df.apply(\n",
    "    lambda r: \"metadata\" \n",
    "              if \"hip hop\" in [g.lower() for g in r[\"MetaGenres\"]]\n",
    "              else \"inferred\",\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c11a16-e28e-4a9d-9d6f-ad5b628af844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# assumes has_meta contains songs with real metadata and 'vector' & 'Bucket' columns\n",
    "X = np.stack(has_meta[\"vector\"].tolist())\n",
    "y = has_meta[\"Bucket\"].values\n",
    "\n",
    "# train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# KNN training\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric=\"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# evaluation\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n",
    "cm = confusion_matrix(y_test, y_pred, labels=knn.classes_)\n",
    "print(pd.DataFrame(cm, index=knn.classes_, columns=knn.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f832a8-3f51-4c0b-9110-23637d5b4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "df[\"MetaGenres_list\"] = df[\"MetaGenres\"].apply(literal_eval)\n",
    "has_meta = df[df[\"MetaGenres_list\"].apply(lambda L: L != [\"OtherMeta\"])].copy()\n",
    "\n",
    "# feature matrix and labels\n",
    "X = np.stack(has_meta[\"vector\"].apply(literal_eval).tolist())\n",
    "y = has_meta[\"Bucket\"].values\n",
    "\n",
    "print(f\"Loaded {X.shape[0]} songs with {X.shape[1]}-dim vectors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8db1f-aa03-4b2c-9fe5-2da8d92f0219",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# cross-validate k values\n",
    "results = []\n",
    "for k in [1, 3, 5, 7, 9, 11]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=\"cosine\")\n",
    "    scores = cross_val_score(knn, X, y, cv=5, scoring=\"accuracy\")\n",
    "    results.append({\"k\": k, \"accuracy\": scores.mean()})\n",
    "\n",
    "print(pd.DataFrame(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e07d54-108a-445e-9533-be0b75a7e7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold-out evaluation for chosen k\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "best_k = 5\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k, metric=\"cosine\")\n",
    "knn.fit(X_tr, y_tr)\n",
    "print(classification_report(y_te, knn.predict(X_te), zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52785a-af4f-4bc1-b7f9-814c089dc485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighted k-NN cross-validation\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric=\"cosine\", weights=\"distance\")\n",
    "scores = cross_val_score(knn, X, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"{scores.mean():.4f} ± {scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e34fe6-fbcb-4340-97ce-cbdf88fcdb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# TF-IDF + Logistic Regression\n",
    "clf = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000, stop_words=\"english\"),\n",
    "    LogisticRegression(max_iter=300, class_weight=\"balanced\")\n",
    ")\n",
    "scores = cross_val_score(\n",
    "    clf,\n",
    "    has_meta[\"Lyrics\"].astype(str),\n",
    "    has_meta[\"Bucket\"],\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "print(f\"{scores.mean():.4f} ± {scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9093cf8-9d6e-425d-90f8-0b7cd65e21af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# TF-IDF + embeddings + SVC\n",
    "tfidf = TfidfVectorizer(max_features=3000, stop_words=\"english\")\n",
    "X_tfidf = tfidf.fit_transform(has_meta[\"Lyrics\"].astype(str))\n",
    "X_emb = sparse.csr_matrix(X)\n",
    "X_combined = sparse.hstack([X_tfidf, X_emb])\n",
    "\n",
    "svc = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "scores = cross_val_score(svc, X_combined, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"{scores.mean():.4f} ± {scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b971d44-2026-4995-9b8f-2e37e32f14ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load data and parse vectors\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "df[\"vector\"] = df[\"vector\"].apply(literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79f95de-131e-4c0e-a981-0cf05d9f1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Filter metadata-rich songs and prepare features\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "has_meta = df[df[\"MetaGenres\"].apply(lambda L: L != [\"OtherMeta\"])].copy()\n",
    "y = has_meta[\"Bucket\"].values\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=3000, stop_words=\"english\", ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(has_meta[\"Lyrics\"].astype(str))\n",
    "X_emb = csr_matrix(np.stack(has_meta[\"vector\"].tolist()))\n",
    "\n",
    "X_hybrid = hstack([X_tfidf, X_emb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2312b08a-9300-4039-a503-a3f8b3b53042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Evaluate hybrid SVM with cross-validation and hold-out\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "svc = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "cv_scores = cross_val_score(svc, X_hybrid, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"CV accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X_hybrid, y, test_size=0.2, random_state=42, stratify=y)\n",
    "svc.fit(X_tr, y_tr)\n",
    "y_pred = svc.predict(X_te)\n",
    "print(classification_report(y_te, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc9155d-72dc-4a81-9632-645770b056cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Load PCA-refined file and parse vectors\n",
    "import pandas as pd\n",
    "\n",
    "df_pca = pd.read_csv(\"F-all_songs_refined_with_PCA.csv\")\n",
    "df_pca[\"vector\"] = df_pca[\"vector\"].apply(literal_eval)\n",
    "\n",
    "if \"Decade\" not in df_pca.columns:\n",
    "    df_pca[\"Year_int\"] = pd.to_datetime(df_pca[\"Year\"], errors=\"coerce\").dt.year\n",
    "    df_pca[\"Decade\"] = (df_pca[\"Year_int\"] // 10 * 10).astype(\"Int64\").astype(str) + \"s\"\n",
    "    df_pca.drop(columns=[\"Year_int\"], inplace=True)\n",
    "\n",
    "has_meta_pca = df_pca[df_pca[\"MetaGenres\"].apply(lambda L: L != [\"OtherMeta\"])].copy()\n",
    "no_meta_pca  = df_pca[df_pca[\"MetaGenres\"].apply(lambda L: L == [\"OtherMeta\"])].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a408c079-61ed-4384-9761-bf4854d3fee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Retrain hybrid SVM on metadata-rich PCA data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "tfidf2 = TfidfVectorizer(max_features=3000, stop_words=\"english\", ngram_range=(1,2))\n",
    "X_tfidf2 = tfidf2.fit_transform(has_meta_pca[\"Lyrics\"].astype(str))\n",
    "X_emb2 = csr_matrix(np.stack(has_meta_pca[\"vector\"].tolist()))\n",
    "X_hybrid2 = hstack([X_tfidf2, X_emb2])\n",
    "\n",
    "y2 = has_meta_pca[\"Bucket\"].values\n",
    "svc2 = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "svc2.fit(X_hybrid2, y2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d5418-b2fa-4588-8fdc-65d95c9fc6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Predict buckets for songs without metadata\n",
    "X_tfidf_no = tfidf2.transform(no_meta_pca[\"Lyrics\"].astype(str))\n",
    "X_emb_no = csr_matrix(np.stack(no_meta_pca[\"vector\"].tolist()))\n",
    "X_hybrid_no = hstack([X_tfidf_no, X_emb_no])\n",
    "\n",
    "preds_no = svc2.predict(X_hybrid_no)\n",
    "no_meta_pca[\"Bucket\"] = preds_no\n",
    "\n",
    "df_pca.loc[no_meta_pca.index, \"Bucket\"] = no_meta_pca[\"Bucket\"]\n",
    "df_pca.to_csv(\"F-all_songs_raw_with_buckets.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1327de3e-87a1-4dbb-9453-51b3e1a19435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Extract year from Release Date and assign Decade\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "df_final = pd.read_csv(\"F-all_songs_refined_with_PCA.csv\")\n",
    "\n",
    "def extract_year(s):\n",
    "    m = re.search(r\"(\\d{4})\", str(s))\n",
    "    return int(m.group(1)) if m else np.nan\n",
    "\n",
    "df_final[\"Year_Extracted\"] = df_final[\"Release Date\"].apply(extract_year)\n",
    "df_final[\"Decade\"] = df_final[\"Year_Extracted\"].apply(lambda y: f\"{int(y//10*10)}s\" if not np.isnan(y) else pd.NA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d7d039-bb7a-4cd3-97f9-6b2289def3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Export balanced 60s–20s\n",
    "decades_60_up = ['1960s','1970s','1980s','1990s','2000s','2010s','2020s']\n",
    "df60 = df_final[df_final[\"Decade\"].isin(decades_60_up)]\n",
    "n60 = df60[\"Decade\"].value_counts().min()\n",
    "balanced60 = df60.groupby(\"Decade\").sample(n=n60, random_state=42)\n",
    "balanced60.to_csv(\"F-balanced_60s_up.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd51bb99-b7a0-4866-b0bc-1c17c48e612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Export balanced 70s–20s\n",
    "decades_70_up = ['1970s','1980s','1990s','2000s','2010s','2020s']\n",
    "df70 = df_final[df_final[\"Decade\"].isin(decades_70_up)]\n",
    "n70 = df70[\"Decade\"].value_counts().min()\n",
    "balanced70 = df70.groupby(\"Decade\").sample(n=n70, random_state=42)\n",
    "balanced70.to_csv(\"F-balanced_70s_up.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c15e90-2095-42b8-adf7-7116c956ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: List, rename, and link final CSVs\n",
    "import glob, os\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "csvs = glob.glob(\"F-*.csv\") + glob.glob(\"Improved_Bucket_Accuracy_*.csv\")\n",
    "print(\"Found these CSVs in\", os.getcwd(), \":\\n\", csvs)\n",
    "for path in csvs:\n",
    "    display(FileLink(path))\n",
    "\n",
    "desired = {\n",
    "    \"Improved_Bucket_Accuracy_all_songs_raw_with_buckets.csv\",\n",
    "    \"Improved_Bucket_Accuracy_balanced_60s_up.csv\",\n",
    "    \"Improved_Bucket_Accuracy_balanced_70s_up.csv\"\n",
    "}\n",
    "for fname in os.listdir('.'):\n",
    "    if fname.startswith(\"Improved_Bucket_Accuracy_\") and fname not in desired:\n",
    "        original = \"F-\" + fname.replace(\"Improved_Bucket_Accuracy_\", \"\", 1)\n",
    "        os.rename(fname, original)\n",
    "        print(f\"Reverted {fname} → {original}\")\n",
    "\n",
    "print(\"\\nFinal CSVs with improved buckets:\")\n",
    "for f in desired:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"- {f}\")\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "\n",
    "for f in desired:\n",
    "    display(FileLink(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf584cc4-ef3e-41ca-8f3d-be88f3a68b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Hold-out evaluation with hybrid SVM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "csv_path = \"Improved_Bucket_Accuracy_all_songs_raw_with_buckets.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df['vector'] = df['vector'].apply(literal_eval)\n",
    "\n",
    "has_meta = df[df['MetaGenres'].apply(lambda L: L != [\"OtherMeta\"])]\n",
    "y = has_meta['Bucket'].values\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=3000, stop_words='english', ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(has_meta['Lyrics'].astype(str))\n",
    "X_emb = csr_matrix(np.stack(has_meta['vector'].tolist()))\n",
    "X = hstack([X_tfidf, X_emb])\n",
    "\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.2,\n",
    "                                         random_state=42, stratify=y)\n",
    "svc = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "svc.fit(X_tr, y_tr)\n",
    "y_pred = svc.predict(X_te)\n",
    "\n",
    "print(\"Hold-out Accuracy:\", accuracy_score(y_te, y_pred))\n",
    "print(classification_report(y_te, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde90b21-6f74-469a-b3da-b0ccfc841e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: KNN cross-validation and hold-out test\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "df['vector'] = df['vector'].apply(literal_eval)\n",
    "has_meta = df[df['MetaGenres'].apply(lambda L: L != [\"OtherMeta\"])]\n",
    "\n",
    "X = np.stack(has_meta['vector'].tolist())\n",
    "y = has_meta['Bucket'].values\n",
    "\n",
    "results = []\n",
    "for k in [1, 3, 5, 7, 9, 11]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k, metric=\"cosine\")\n",
    "    scores = cross_val_score(knn, X, y, cv=5, scoring=\"accuracy\")\n",
    "    results.append({'k': k, 'accuracy': scores.mean()})\n",
    "print(pd.DataFrame(results))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric=\"cosine\")\n",
    "knn.fit(X_train, y_train)\n",
    "print(classification_report(y_test, knn.predict(X_test), zero_division=0))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, metric=\"cosine\", weights=\"distance\")\n",
    "print(\"Weighted k-NN CV accuracy:\", cross_val_score(knn, X, y, cv=5, scoring=\"accuracy\").mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c11b7-8c02-4fd4-bc43-9370f250509b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: TF-IDF + Logistic Regression and TF-IDF+Embeddings+SVC\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy import sparse\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "df['vector'] = df['vector'].apply(literal_eval)\n",
    "has_meta = df[df['MetaGenres'].apply(lambda L: L != [\"OtherMeta\"])]\n",
    "\n",
    "# TF-IDF + Logistic Regression\n",
    "clf = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000, stop_words=\"english\"),\n",
    "    LogisticRegression(max_iter=300, class_weight=\"balanced\")\n",
    ")\n",
    "print(\"TF-IDF + LR CV accuracy:\",\n",
    "      cross_val_score(clf, has_meta['Lyrics'].astype(str),\n",
    "                      has_meta['Bucket'], cv=5, scoring=\"accuracy\").mean())\n",
    "\n",
    "# TF-IDF + Embeddings + SVC\n",
    "tfidf2 = TfidfVectorizer(max_features=3000, stop_words=\"english\")\n",
    "X_tfidf2 = tfidf2.fit_transform(has_meta['Lyrics'].astype(str))\n",
    "X_emb2 = sparse.csr_matrix(np.stack(has_meta['vector'].tolist()))\n",
    "X_combined = sparse.hstack([X_tfidf2, X_emb2])\n",
    "\n",
    "svc2 = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "print(\"TF-IDF + Embeddings + SVC CV accuracy:\",\n",
    "      cross_val_score(svc2, X_combined, has_meta['Bucket'], cv=5, scoring=\"accuracy\").mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb8db71-d386-4528-b031-006d2fd359a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Hybrid SVM on regex‐decoded data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1) Load your data and parse the stored vectors\n",
    "df = pd.read_csv(\"F-all_songs_with_regex_decade.csv\")\n",
    "df['vector'] = df['vector'].apply(literal_eval)\n",
    "\n",
    "# 2) Keep only the songs with real metadata (ground truth)\n",
    "has_meta = df[df['MetaGenres'].apply(lambda L: L != [\"OtherMeta\"])].copy()\n",
    "y = has_meta['Bucket'].values\n",
    "\n",
    "# 3) Build TF–IDF features from lyrics\n",
    "tfidf = TfidfVectorizer(max_features=3000, stop_words='english', ngram_range=(1,2))\n",
    "X_tfidf = tfidf.fit_transform(has_meta['Lyrics'].astype(str))\n",
    "\n",
    "# 4) Build embedding features (your 384-dim vectors)\n",
    "X_emb = csr_matrix(np.stack(has_meta['vector'].tolist()))\n",
    "\n",
    "# 5) Combine into the hybrid feature matrix\n",
    "X_hybrid = hstack([X_tfidf, X_emb])\n",
    "\n",
    "# Option A: 5-fold CV accuracy\n",
    "svc = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "cv_scores = cross_val_score(svc, X_hybrid, y, cv=5, scoring=\"accuracy\")\n",
    "print(f\"Hybrid SVM CV accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}\")\n",
    "\n",
    "# Option B: Hold-out split + full report\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    X_hybrid, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "svc.fit(X_tr, y_tr)\n",
    "y_pred = svc.predict(X_te)\n",
    "print(\"\\nHybrid SVM hold-out classification report:\\n\")\n",
    "print(classification_report(y_te, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f4949-55c4-435a-b7df-b457fe118764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load PCA-refined data & split metadata/no-metadata\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "# Adjust filename if needed\n",
    "df = pd.read_csv(\"F-all_songs_refined_with_PCA.csv\")\n",
    "df['vector'] = df['vector'].apply(literal_eval)\n",
    "\n",
    "# If Decade not yet derived, do it now\n",
    "if 'Decade' not in df.columns:\n",
    "    df['Year_int'] = pd.to_datetime(df['Year'], errors='coerce').dt.year\n",
    "    df['Decade']  = (df['Year_int'] // 10 * 10).astype('Int64').astype(str) + 's'\n",
    "    df.drop(columns=['Year_int'], inplace=True)\n",
    "\n",
    "print(\"Loaded DataFrame:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# has_meta = songs with real Spotify genres  \n",
    "has_meta = df[df['MetaGenres'].apply(lambda L: L != [\"OtherMeta\"])].copy()\n",
    "# no_meta  = songs we will predict via the hybrid SVM\n",
    "no_meta  = df[df['MetaGenres'].apply(lambda L: L == [\"OtherMeta\"])].copy()\n",
    "\n",
    "print(\"has_meta rows:\", has_meta.shape[0])\n",
    "print(\"no_meta rows: \", no_meta.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca181f-ef70-459f-b65d-915312216477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Retrain hybrid SVM on metadata-rich PCA data, predict missing, export\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# 1) TF–IDF on has_meta lyrics\n",
    "tfidf2 = TfidfVectorizer(max_features=3000, stop_words='english', ngram_range=(1,2))\n",
    "X_tfidf2 = tfidf2.fit_transform(has_meta['Lyrics'].astype(str))\n",
    "\n",
    "# 2) Embedding matrix from precomputed vectors\n",
    "X_emb2 = csr_matrix(np.stack(has_meta['vector'].tolist()))\n",
    "\n",
    "# 3) Hybrid feature matrix\n",
    "X_hybrid2 = hstack([X_tfidf2, X_emb2])\n",
    "\n",
    "# 4) Train SVM\n",
    "y2 = has_meta['Bucket'].values\n",
    "svc2 = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "svc2.fit(X_hybrid2, y2)\n",
    "print(\"Hybrid SVM trained on metadata-rich songs\")\n",
    "\n",
    "# 5) Predict buckets for no_meta\n",
    "X_tfidf_no = tfidf2.transform(no_meta['Lyrics'].astype(str))\n",
    "X_emb_no   = csr_matrix(np.stack(no_meta['vector'].tolist()))\n",
    "X_hybrid_no = hstack([X_tfidf_no, X_emb_no])\n",
    "\n",
    "preds_no = svc2.predict(X_hybrid_no)\n",
    "no_meta['Bucket'] = preds_no\n",
    "\n",
    "# 6) Merge back and export\n",
    "df.loc[no_meta.index, 'Bucket'] = no_meta['Bucket']\n",
    "df.to_csv(\"F-all_songs_raw_with_buckets.csv\", index=False)\n",
    "print(\"Exported F-all_songs_raw_with_buckets.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea54b2a-16bb-4fc4-9608-7b67ce0b896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Extract year from Release Date, assign Decade, export balanced sets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# 1) Load final CSV\n",
    "df_final = pd.read_csv(\"F-all_songs_raw_with_buckets.csv\")\n",
    "\n",
    "# 2) Extract first 4-digit year\n",
    "def extract_year(s):\n",
    "    m = re.search(r\"(\\d{4})\", str(s))\n",
    "    return int(m.group(1)) if m else np.nan\n",
    "\n",
    "df_final['Year_Extracted'] = df_final['Release Date'].apply(extract_year)\n",
    "\n",
    "# 3) Map to decades\n",
    "df_final['Decade'] = df_final['Year_Extracted'].apply(\n",
    "    lambda y: f\"{int(y//10*10)}s\" if not np.isnan(y) else \"Unknown\"\n",
    ")\n",
    "\n",
    "print(df_final['Decade'].value_counts(dropna=False).sort_index())\n",
    "\n",
    "# 4) Export balanced 60s–2020s\n",
    "dec60 = ['1960s','1970s','1980s','1990s','2000s','2010s','2020s']\n",
    "df60 = df_final[df_final['Decade'].isin(dec60)]\n",
    "n60 = df60['Decade'].value_counts().min()\n",
    "balanced60 = df60.groupby('Decade').sample(n=n60, random_state=42)\n",
    "balanced60.to_csv(\"F-balanced_60s_up.csv\", index=False)\n",
    "\n",
    "# 5) Export balanced 70s–2020s\n",
    "dec70 = ['1970s','1980s','1990s','2000s','2010s','2020s']\n",
    "df70 = df_final[df_final['Decade'].isin(dec70)]\n",
    "n70 = df70['Decade'].value_counts().min()\n",
    "balanced70 = df70.groupby('Decade').sample(n=n70, random_state=42)\n",
    "balanced70.to_csv(\"F-balanced_70s_up.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ff146-7183-4601-a975-8de19f2f56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: List & link your final CSVs\n",
    "import glob, os\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "csvs = glob.glob(\"F-*.csv\") + glob.glob(\"Improved_Bucket_Accuracy_*.csv\")\n",
    "print(\"CSV files found:\", csvs)\n",
    "for c in csvs:\n",
    "    display(FileLink(c))\n",
    "\n",
    "desired = {\n",
    "    \"Improved_Bucket_Accuracy_all_songs_raw_with_buckets.csv\",\n",
    "    \"Improved_Bucket_Accuracy_balanced_60s_up.csv\",\n",
    "    \"Improved_Bucket_Accuracy_balanced_70s_up.csv\"\n",
    "}\n",
    "for fname in os.listdir('.'):\n",
    "    if fname.startswith(\"Improved_Bucket_Accuracy_\") and fname not in desired:\n",
    "        os.rename(fname, \"F-\" + fname.replace(\"Improved_Bucket_Accuracy_\", \"\", 1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
